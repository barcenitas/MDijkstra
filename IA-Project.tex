\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{vmargin}
\usepackage{afterpage}
\author{Elaborado por:\\\\Bárcenas Martínez Erick Iván \\ Bernal Sánchez Diego Arturo \\ Rivera Negrete Manuel Armando\\\\Titular:Rodrigo Castillo}
\date{10 de Junio del 2018}
\title{Universidad Nacional Autónoma de México\\Facultad de Ingeniería\\Programa de Tecnología en Cómputo\\Proyecto de Inteligencia Artificial en MatLab}
\begin{document}
\maketitle
\part{Introducción}
\part{Desarrollo}
\chapter{¿Qué es la inteligencia artificial?}
La Inteligencia Artificial tiene que ver con el diseño de  programas inteligentes, algunas definiciones se refiere a procesos mentales y al razonamiento, otras a la conducta y por último a la  función deseable en función de la eficiencia humana. A lo largo de la historia se han seguido los cuatro enfoques mencionados. Como es
de esperar, existe un enfrentamiento entre los enfoques centrados en los humanos y los
centrados en torno a la racionalidad. De acuerdo con autores recientes la Inteligencia Artificial es: \\``El arte de crear Máquinas con capacidad de realizar funciones que realizadas por personas requieren de inteligencia" (Kurzweil, 1990.)\\``La interesante tarea de lograr que las computadoras piensen ...\textit{máquinas con mente.} en su amplio sentido literal." (Haugeland, 1985.)\\``El estudio de las facultades mentales mediante el uso de modelos computacionales." (Charniak y McDermott, 1985.)\\``La rama de la ciencia de la computación que se ocupa de la automatización de la conducta inteligente." (Luger y Stubblefield, 1993.)
\section{Enfoques de la inteligenca Artificial}
\subsection*{Actuar como humano: Enfoque cognitivo}
Para poder decir que un programa determinado utiliza algún tipo de razonamiento humano es necesario previamente saber como piensan los seres humanos. Hay dos formas de hacerlo: mediante introspección (intentando atrapar nuestros propios pensamientos conforme éstos van apareciendo) y mediante experimentos psicológicos. Una vez se cuente con una teoría lo
suficientemente precisa sobre cómo trabaja la mente, se podrá expresar esa teoría en la
forma de un programa de computador. Si los datos de entrada/salida del programa y los
tiempos de reacción son similares a los de un humano, existe la evidencia de que algunos de los mecanismos del programa se pueden comparar con los que utilizan los seres
humanos.
\subsection*{Pensar como humano: Enfoque por las \textit{ ``leyes del pensamiento"}}
Estudiosos de la lógica desarrollaron, en el siglo XIX , una notación precisa para definir sentencias sobre todo tipo de elementos del mundo y especificar relaciones entre ellos (compárese esto con la notación aritmética común, que prácticamente sólo sirve
para representar afirmaciones acerca de la igualdad y desigualdad entre números). Ya
en 1965 existían programas que, en principio, resolvían cualquier problema resoluble
descrito en notación lógica.\\ La llamada tradición logista dentro del campo de la inteliencia artificial trata de construir sistemas inteligentes a partir de estos programas. Este enfoque presenta dos obstáculos. No es fácil transformar conocimiento informal y expresarlo en los términos formales que requieren de notación lógica, particularmente cuando el conocimiento que se tiene es inferior al 100 por 100. En segundo lugar, hay una gran diferencia entre poder resolver un problema «en principio» y hacerlo en la práctica. Incluso problemas con apenas una docena de datos pueden agotar los recursos computacionales de cualquier computador a menos que cuente con alguna directiva sobre los pasos de razonamiento que hay que llevar a cabo primero.\\ Aunque los dos obstáculos anteriores están presentes en todo intento de construir sistemas de razonamiento computacional, surgieron por primera vez en la tradición lógica.
\subsection*{Actuar de manera racional: Enfoque por agentes racionales}
Actuar racionalmente implica actuar de manera tal que se logren los objetivos deseados, con base en ciertos supuestos. Un agente es algo capaz de percibir y actuar.\\En el caso del enfoque de la IA según ``las leyes del pensamiento", todo el énfasis se ponía en hacer inferencias correctas. La obtención de estas inferencia a veces \textit{forma parte} de lo que se considera un agente rracional, puesto que una manera de actuar racionalmente es el razonamiento lógico que nos asegure la obtención de un resultado determinado.\\Todas las ``habilidades cognoscitivas" que se necesitan en la prueba de Turing permiten emprender conocimiento y razonar con base en él, pues de esta manera se podrán tomar deciciones correctas en una amplia gama de sitauciones.\\La percepción visualno es sólo algo divertudo, sino que es algo necesario para darse una mejor idea de lo que una acción determinada puede producir.\\Estudiar la IA adoptando el enfoque del diseño de un agente racional ofrece dos ventajas. Primera, es más general que el enfoque de las ``Leyes del pensamineto", dado que el efectuar inferencias correctas es sólo uno de los mecanismos existentes para garantizar la racionalidad, pero no es un mecanismo necesario. La segunda es más afín a la forma en la que se ha producido el avance científico que los enfoques basados en la conducta o pensamiento humano, porque la norma de la racionalidad está claramente definida y es de aplicación general. Por el contrario, la conducta humana se adapta bien a un entorno específico, y en parte, es producto de un proceso evolutivo complejo, en gran medida desconocido, que aún está lejos de llevarnos a la perfección
\section{Fundamentos de la inteligencia artificial}
\subsection*{Filosofía}
Aristóteles (384-322 a.C.) fue el primero en formular un conjunto preciso de leyes que gobernaban la parte racional de la inteligencia. Él desarrolló un sistema informal para razonar adecuadamente con silogismos, que en principio permitía extraer conclusiones mecánicamente, a partir de premisas iniciales. Mucho después, Ramón Lull (d. 1315) tuvo la idea de que el razonamiento útil se podría obtener por medios artificiales. Sus «ideas» aparecen representadas en la portada de este manuscrito. Thomas Hobbes (1588-1679) propuso que el razonamiento era como la computación numérica, de forma que «nosotros sumamos y restamos silenciosamente en nuestros pensamientos». La automatización de la computación en sí misma estaba en marcha; alrededor de 1500, Leonardo da Vinci (1452-1519) diseñó, aunque no construyó, una calculadora mecánica; construcciones recientes han mostrado que su diseño era funcional. La primera máquina calculadora conocida se construyó alrededor de 1623 por el científico alemán Wilhelm Schickard (1592-1635), aunque la Pascalina, construida en 1642 por Blaise Pascal (1623-1662), sea más famosa. Pascal escribió que «lamáquina aritmética produce efectos que parecen más similares a los pensamientos que a las acciones animales». Gottfried Wilhelm Leibniz (1646-1716) construyó un dispositivo mecánico con el objetivo de llevar a cabo operaciones sobre conceptos en lugar de sobre números, pero su campo de acción era muy limitado\\René Descartes (1596-1650) proporciona la primera discusión clara sobre la distinción entre la mente y la materia y los problemas que surgen.\\Descartes sostenía que existe una parte de la mente (o del alma o del espíritu) que está al margen de la naturaleza, exenta de la influencia de las leyes físicas. Los animales, por el contrario, no poseen esta cualidad dual; a ellos se le podría concebir como si se tratasen de máquinas. Una alternativa al dualismo es el materialismo, que considera que las operaciones del cerebro realizadas de acuerdo a las leyes de la física constituyen la mente. El libre albedrío es simplemente la forma en la que la percepción de las opciones disponibles aparecen en el proceso de selección.
\subsection*{Matemáticas}
Los filósofos delimitaron las ideas más importantes de la IA, pero para pasar de ahí a una ciencia formal es necesario contar con una formulación matemática en tres áreas fundamentales: lógica, computación y probabilidad. Se piensa que el primer algoritmo no trivial es el algoritmo Euclídeo para el cálculo del máximo común divisor.\\Se dice que un problema es intratable si el tiempo necesario para la resolución de casos particulares de dicho problema crece exponencialmente con el tamaño de dichos casos. La diferencia entre crecimiento polinomial y exponencial de la complejidad se destacó por primera vez a mediados de los años 60\\
Es importante porque un crecimiento exponencial implica la imposibilidad de resolver casos moderadamente grandes en un tiempo razonable. Por tanto, se debe optar por dividir el problema de la generación de una conducta inteligente en subproblemas que sean tratables en vez de manejar problemas intratables.\\
La teoría de la NP-completitud, propuesta por primera vez por Steven Cook (1971) y Richard Karp (1972) propone un método. Cook y Karp demostraron la existencia de grandes clases de problemas de razonamiento y búsqueda combinatoria canónica que son NP completos. Toda clase de problema a la que la clase de problemas NP completos se pueda reducir será seguramente intratable (aunque no se ha demostrado que los problemas NP completos son necesariamente intratables, la mayor parte de los teóricos así lo creen).
Además de la lógica y el cálculo, la tercera gran contribución de las matemáticas a la IA es la teoría de la probabilidad. La probabilidad se convirtió pronto en parte imprescindible de las ciencias cuantitativas, ayudando en el tratamiento de mediciones con incertidumbre y de teorías incompletas.
\subsection*{Economía}
La teoría de la decisión, que combina la teoría de la probabilidad con la teoría de la utilidad, proporciona un marco completo y formal para la toma de decisiones (económicas o de otra índole) realizadas bajo incertidumbre, esto es, en casos en los que las descripciones probabilísticas capturan adecuadamente la forma en la que se toman las decisiones en el entorno; lo cual es adecuado para «grandes» economías en las que cada agente no necesita prestar atención a las acciones que lleven a cabo el resto de los agentes individualmente.\\
El trabajo en la economía y la investigación operativa ha contribuido en gran medida a la noción de agente racional que aquí se presenta, aunque durante muchos años la investigación en el campo de la IA se ha desarrollado por sendas separadas. Una razón fue la complejidad aparente que trae consigo el tomar decisiones racionales. Herbert Simon (1916-2001), uno de los primeros en investigar en el campo de la IA, ganó el premio Nobel en Economía en 1978 por su temprano trabajo, en el que mostró que los modelos basados en satisfacción (que toman decisiones que son «suficientemente buenas», en vez de realizar cálculos laboriosos para alcanzar decisiones óptimas) proporcionaban una descripción mejor del comportamiento humano real (Simon, 1947). En los años 90, hubo un resurgimiento del interés en las técnicas de decisión teórica para sistemas basados en agentes (Wellman, 1995).
\subsection*{Neurociencia}
La Neurociencia es el estudio del sistema neurológico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. Se ha observado durante miles de años que el cerebro está de alguna manera involucrado en los procesos de pensamiento, ya que fuertes golpes en la cabeza pueden ocasionar minusvalía mental.\\
No fue hasta mediados del siglo XVIII cuando
se aceptó mayoritariamente que el cerebro es la base de la conciencia. Hasta este momento, se pensaba que estaba localizado en el corazón, el bazo y la glándula pineal.\\
La conclusión verdaderamente increíble es que una colección de simples células puede llegar a generar razonamiento, acción, y conciencia o, dicho en otras palabras, los cerebros generan las inteligencias (Searle, 1992). La única teoría alternativa es el misticismo: que nos dice que existe alguna esfera mística en la que las mentes operan fuera del control de la ciencia física.\\
Cerebros y computadores digitales realizan tareas bastante diferentes y tienen propiedades distintas. Hay 1.000 veces más neuronas en un cerebro humano medio que puertas lógicas en la UCP de un computador estándar. La ley de Moore predice que el número de puertas lógicas de la UCP se igualará con el de neuronas del cerebro alrededor del año 2020. Por supuesto, poco se puede inferir de esta predicción; más aún, la diferencia en la capacidad de almacenamiento es insignificante comparada con las diferencias en la velocidad de intercambio y en paralelismo. Los circuitos de los computadores pueden ejecutar una instrucción en un nanosegundo, mientras que las neuronas son millones de veces más lentas. Las neuronas y las sinapsis del cerebro están activas simultáneamente, mientras que los computadores actuales tienen una o como mucho varias UCP. Por tanto, incluso sabiendo que un computador es un millón de veces más rápido en cuanto a su velocidad de intercambio, el cerebro acaba siendo 100,000 veces más rápido en lo que hace.
\subsection*{Psicología}
La conceptualización del cerebro como un dispositivo de procesamiento de información, característica principal de la psicología cognitiva, se remonta por lo menos a las obras de William James 10 (1842-1910). Helmholtz también pone énfasis en que la percepción entraña cierto tipo de inferencia lógica inconsciente. Este punto de vista cognitivo se vio eclipsado por el conductismo en Estados Unidos, pero en la Unidad de Psicología Aplicada de Cambridge, dirigida por Frederic Bartlett (1886-1969), los modelos cognitivos emergieron con fuerza. La obra The Nature of Explanation, de Kenneth Craik (1943), discípulo y sucesor de Bartlett, reestablece enérgicamente la legitimidad de términos «mentales» como creencias y objetivos, argumentando que son tan científicos como lo pueden ser la presión y la temperatura cuando se habla acerca de los gases, a pesar de que éstos estén formados por moléculas que no tienen ni presión ni temperatura. Craik establece tres elementos clave que hay que tener en cuenta para diseñar un agente basado en conocimiento: (1) el estímulo deberá ser traducido a una representación interna, (2) esta representación se debe manipular mediante procesos cognitivos para así generar nuevas representaciones internas, y (3) éstas, a su vez, se traducirán de nuevo en acciones. Dejó muy claro por qué consideraba que estos eran los requisitos idóneos para diseñar un agente: Si el organismo tiene en su cabeza «un modelo a pequeña escala» de la realidad externa y de todas sus posibles acciones, será capaz de probar diversas opciones, decidir cuál es la mejor, planificar su reacción ante posibles situaciones futuras antes de que éstas surjan, emplear lo aprendido de experiencias pasadas en situaciones presentes y futuras, y en todo momento, reaccionar ante los imprevistos que acontezcan de manera satisfactoria, segura y más competente (Craik, 1943).\\
Los psicólogos comparten en la actualidad el punto de vista común de que «la teoría cognitiva debe ser como un programa de computador» (Anderson, 1980), o dicho de otra forma, debe describir un mecanismo de procesamiento de información detallado, lo cual lleva consigo la implementación de algunas funciones cognitivas.
\subsection*{Ingeniería en computación}
Para que la inteligencia artificial pueda llegar a ser una realidad se necesitan dos cosas: inteligencia y un artefacto. El computador ha sido el artefacto elegido. El computador electrónico digital moderno se inventó de manera independiente y casi simultánea por científicos en tres países involucrados en la Segunda Guerra Mundial. El equipo de Alan Turing construyó, en 1940, el primer computador operacional de carácter electromecánico, llamado Heath Robinson, con un único propósito: descifrar mensajes alemanes.\\
Desde mediados del siglo pasado, cada generación de dispositivos hardware ha conllevado un aumento en la velocidad de proceso y en la capacidad de almacenamiento, así como una reducción de precios. La potencia de los computadores se dobla cada 18 meses aproximadamente y seguirá a este ritmo durante una o dos décadas más. Después, se necesitará ingeniería molecular y otras tecnologías novedosas.\\
La IA también tiene una deuda con la parte software de la informática que ha proporcionado los sistemas operativos, los lenguajes de programación, y las herramientas necesarias para escribir programas modernos (y artículos sobre ellos). Sin embargo, en este área la deuda se ha saldado: la investigación en IA ha generado numerosas ideas novedosas de las que se ha beneficiado la informática en general, como por ejemplo el tiempo compartido, los intérpretes imperativos, los computadores personales con interfaces gráficas y ratones, entornos de desarrollo rápido, listas enlazadas, administración automática de memoria, y conceptos claves de la programación simbólica, funcional, dinámica y orientada a objetos.
\subsection*{Teoría de control y cibernética}
Ktesibios de Alejandría (250 a.C.) construyó la primera máquina auto controlada: un reloj de agua con un regulador que mantenía el flujo de agua circulando por él, con un ritmo constante y predecible. Esta invención cambió la definición de lo que un artefacto podía hacer.\\La teoría de control moderna, especialmente la rama conocida como control óptimo estocástico, tiene como objetivo el diseño de sistemas que maximizan una función objetivo en el tiempo. Lo cual se asemeja ligeramente a nuestra visión de lo que es la IA: diseño de sistemas que se comportan de forma óptima. ¿Por qué, entonces, IA y teoría de control son dos campos diferentes, especialmente teniendo en cuenta la cercana relación entre sus creadores? La respuesta está en el gran acoplamiento existente entre las técnicas matemáticas con las que estaban familiarizados los investigadores y entre los conjuntos de problemas que se abordaban desde cada uno de los puntos de vista. El cálculo y el álgebra matricial, herramientas de la teoría de control, se utilizaron en la definición de sistemas que se podían describir mediante conjuntos fijos de variables continuas; más aún, el análisis exacto es sólo posible en sistemas lineales. La IA se fundó en parte para escapar de las limitaciones matemáticas de la teoría de control en los años 50. Las herramientas de inferencia lógica y computación permitieron a los investigadores de IA afrontar problemas relacionados con el lenguaje, visión y planificación, que estaban completamente fuera del punto de mira de la teoría de control.
\subsection*{Lingüística}
La lingüística moderna y la IA «nacieron», al mismo tiempo y maduraron juntas, solapándose en un campo híbrido llamado lingüística computacional o procesamiento del lenguaje natural. El problema del entendimiento del lenguaje se mostró pronto mucho más complejo de lo que se había pensado en 1957. El entendimiento del lenguaje requiere la comprensión de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias. Lo cual puede parecer obvio, pero no lo fue para la mayoría de la comunidad investigadora hasta los años 60. Gran parte de los primeros trabajos de investigación en el área de la representación del conocimiento (el estudio de cómo representar el conocimiento de forma que el computador pueda razonar a partir de dicha representación) estaban vinculados al lenguaje y a la búsqueda de información en el campo del lenguaje, y su base eran las investigaciones realizadas durante décadas en el análisis filosófico del lenguaje.
\section{Historia de la inteligencia artificial}
\subsection*{Gestión de la inteligencia artificial (1943-1955)}
Warren McCulloch y Walter Pitts (1943) han sido reconocidos como los autores del primer trabajo de IA. Partieron de tres fuentes: conocimientos sobre la fisiología básica y funcionamiento de las neuronas en el cerebro, el análisis formal de la lógica proposicional de Russell y Whitehead y la teoría de la computación de Turing. Propusieron un modelo constituido por neuronas artificiales, en el que cada una de ellas se caracterizaba por estar «activada» o «desactivada»; la «activación» se daba como respuesta a la estimulación producida por una cantidad suficiente de neuronas vecinas. El estado de una neurona se veía como «equivalente, de hecho, a una proposición con unos estímulos adecuados». Mostraron, por ejemplo, que cualquier función de cómputo podría calcularse mediante alguna red de neuronas interconectadas, y que todos los conectores lógicos (and, or, not, etc.) se podrían implementar utilizando estructuras de red sencillas.\\
Hay un número de trabajos iniciales que se pueden caracterizar como de IA, pero fue Alan Turing quien articuló primero una visión de la IA en su artículo \textit{Computing Machinery and Intelligence}, en 1950. Ahí, introdujo la prueba de Turing, el aprendizaje automático, los algoritmos genéricos y el aprendizaje por refuerzo.
\subsection*{Nacimiento de la inteligencia artificial (1956)}

Princeton acogió a otras de la figuras señeras de la IA, John McCarthy. Posteriormente a su graduación, McCarthy se transladó al Dartmouth College, que se erigiría en el lugar del nacimiento oficial de este campo. McCarthy convenció a Minsky, Claude Shannon y Nathaniel Rochester para que le ayudaran a aumentar el interés de los investigadores americanos en la teoría de autómatas, las redes neuronales y el estudio de la inteligencia. Organizaron un taller con una duración de dos meses en Darmouth en el verano de 1956. Hubo diez asistentes en total, entre los que se incluían Trenchard More de Princeton, Arthur Samuel de IBM, y Ray Solomonoff y Oliver Selfridge del MIT.\\Dos investigadores del Carnegie Tech 13 , Allen Newell y Herbert Simon, acapararon la atención. Si bien los demás también tenían algunas ideas y, en algunos casos, programas para aplicaciones determinadas como el juego de damas, Newell y Simon contaban ya con un programa de razonamiento, el Teórico Lógico (TL), del que Simon afirmaba: «Hemos inventado un programa de computación capaz de pensar de manera no numérica, con lo que ha quedado resuelto el venerable problema de la dualidad mente-cuerpo»\\Poco después del término del taller, el programa ya era capaz de demostrar teoremas.

\subsection*{Entusiasmo temprano, grandes expectaciones (1952-1969)}

Los primeros años de la IA estuvieron llenos de éxitos (aunque con ciertas limitaciones). Teniendo en cuenta lo primitivo de los computadores y las herramientas de programación de aquella época, y el hecho de que sólo unos pocos años antes, a los computadores se les consideraba como artefactos que podían realizar trabajos aritméticos y nadamás, resultó sorprendente que un computador hiciese algo remotamente inteligente. La comunidad científica, en su mayoría, prefirió creer que «una máquina nunca podría hacer tareas»\\Al temprano éxito de Newell y Simon siguió el del sistema de resolución general de problemas, o SRGP. A diferencia del Teórico Lógico, desde un principio este programa se diseñó para que imitara protocolos de resolución de problemas de los seres humanos. Dentro del limitado número de puzles que podía manejar, resultó que la secuencia en la que el programa consideraba que los subobjetivos y las posibles acciones eran semejantes a la manera en que los seres humanos abordaban los mismos problemas. Es decir, el SRGP posiblemente fue el primer programa que incorporó el enfoque de «pensar como un ser humano»
\subsection*{Sistemas Expertos (1969-1979)}

Feigenbaum junto con otros investigadores de Stanford dieron comienzo al Proyecto de Programación Heurística, PPH, dedicado a determinar el grado con el que la nueva metodología de los sistemas expertos podía aplicarse a otras áreas de la actividad humana. El siguiente gran esfuerzo se realizó en el área del diagnóstico médico. Feigenbaum, Buchanan y el doctor Edward Shortliffe diseñaron el programa MYCIN, para el diagnóstico de infecciones sanguíneas. Con 450 reglas aproximadamente, MYCIN era capaz de hacer diagnósticos tan buenos como los de un experto y, desde luego, mejores que los de un médico recién graduado. Se distinguía de DENDRAL en dos aspectos principalmente. En primer lugar, a diferencia de las reglas de DENDRAL, no se contaba con un modelo teórico desde el cual se pudiesen deducir las reglas de MYCIN. Fue necesario obtenerlas a partir de extensas entrevistas con los expertos, quienes las habían obtenido de libros de texto, de otros expertos o de su experiencia directa en casos prácticos. En segundo lugar, las reglas deberían reflejar la incertidumbre inherente al conocimiento médico. MYCIN contaba con un elemento que facilitaba el cálculo de incertidumbre denominado factores de certeza que al parecer (en aquella época) correspondía muy bien a la manera como los médicos ponderaban las evidencias al hacer un diagnóstico\\
La importancia del conocimiento del dominio se demostró también en el área de la comprensión del lenguaje natural. Aunque el sistema SHRDLU de Winograd para la comprensión del lenguaje natural había suscitado mucho entusiasmo, su dependencia del aná- lisis sintáctico provocó algunos de los mismos problemas que habían aparecido en los trabajos realizados en la traducción automática. Era capaz de resolver los problemas de ambigüedad e identificar los pronombres utilizados, gracias a que se había diseñado es- pecialmente para un área (el mundo de los bloques). Fueron varios los investigadores que, como Eugene Charniak, estudiante de Winograd en el MIT, opinaron que para una sólida comprensión del lenguaje era necesario contar con un conocimiento general sobre el mundo y un método general para usar ese conocimiento.
\subsection*{La IA se vuelve industria (1980-presente)}

El primer sistema experto comercial que tuvo éxito, R1, inició su actividad en Digital Equipment Corporation (McDermott, 1982). El programa se utilizaba en la elaboración de pedidos de nuevos sistemas informáticos. En 1986 representaba para la compañía un ahorro estimado de 40 millones de dólares al año. En 1988, el grupo de Inteligencia Artificial de DEC había distribuido ya 40 sistemas expertos, y había más en camino. Du Pont utilizaba ya 100 y estaban en etapa de desarrollo 500 más, lo que le generaba ahorro de diez millones de dólares anuales aproximadamente. Casi todas las compañías importantes de Estados Unidos contaban con su propio grupo de IA, en el que se utilizaban o investigaban sistemas expertos. En 1981 los japoneses anunciaron el proyecto «Quinta Generación», un plan de diez años para construir computadores inteligentes en los que pudiese ejecutarse Prolog. Como respuesta Estados Unidos constituyó la Microelectronics and Computer Technology Corporation (MCC), consorcio encargado de mantener la competitividad nacional en estas áreas. En ambos casos, la IA formaba parte de un gran proyecto que incluía el diseño de chips y la investigación de la relación hombre máquina. Sin embargo, los componentes de IA generados en el marco de MCC y del proyecto Quinta Generación nunca alcanzaron sus objetivos. En el Reino Unido, el informe Alvey restauró el patrocinio suspendido por el informe Lighthill

\subsection*{El regreso de las redes neuronales (1986-presente)}

Aunque la informática había abandonado de manera general el campo de las redes neuronales a finales de los años 70, el trabajo continuó en otros campos. Físicos como John Hopfield (1982) utilizaron técnicas de la mecánica estadística para analizar las propiedades de almacenamiento y optimización de las redes, tratando colecciones de nodos como colecciones de átomos. Psicólogos como David Rumelhart y Geoff Hinton continuaron con el estudio de modelos de memoria basados en redes neuronales.\\El impulso más fuerte se produjo a mediados de la década de los 80, cuando por lo menos cuatro grupos distintos reinventaron el algoritmo de aprendizaje de retroalimentación, mencionado por vez primera en 1969 por Bryson y Ho. El algoritmo se aplicó a diversos problemas de aprendizaje en los campos de la informática y la psicología, y la gran difusión que conocieron los resultados obtenidos, publicados en la colección Parallel Distributed Processing (Rumelhart y McClelland, 1986), suscitó gran entusiasmo. Aquellos modelos de inteligencia artificial llamados conexionistas fueron vistos por algunos como competidores tanto de los modelos simbólicos propuestos por Newell y Simon como de la aproximación lógica de McCarthy entre otros (Smolensky, 1988).

\subsection*{La IA adopta el método científico (1987-presente)}

En los primeros años de la IA parecía perfectamente posible que las nuevas formas de la computación simbólica, por ejemplo, los marcos y las redes semánticas, hicieran que la mayor parte de la teoría clásica pasara a ser obsoleta. Esto llevó a la IA a una especie de aislamiento, que la separó del resto de las ciencias de la computación. En la actualidad se está abandonando este aislamiento. Existe la creencia de que el aprendizaje automático no se debe separar de la teoría de la información, que el razonamiento incierto no se debe separar de los modelos estocásticos, de que la búsqueda no se debe aislar de la optimización clásica y el control, y de que el razonamiento automático no se debe separar de los métodos formales y del análisis estático.\\En términos metodológicos, se puede decir, con rotundidad, que la IA ya forma parte del ámbito de los métodos científicos. Para que se acepten, las hipótesis se deben someter a rigurosos experimentos empíricos, y los resultados deben analizarse estadísticamente para identificar su relevancia (Cohen, 1995). El uso de Internet y el compartir repositorios de datos de prueba y código, ha hecho posible que ahora se puedan contrastar experimentos.
\subsection*{Surgimiento de los agentes inteligentes (1995-presente)}
El «movimiento situado» intenta entender la forma de actuar de los agentes inmersos en entornos reales, que disponen de sensores de entradas continuas. Uno de los medios más importantes para los agentes inteligentes es Internet. Los sistemas de IA han llegado a ser tan comunes en aplicaciones desarrolladas para la Web que el sufijo «-bot» se ha introducido en el lenguaje común. Más aún, tecnologías de IA son la base de muchas herramientas para Internet, como por ejemplo motores de búsqueda, sistemas de recomendación, y los sistemas para la construcción de portales Web.
\subsection*{Grandes bancos de datos (Big Data) (2001-presente)}

La Inteligencia Artificial en toda su magnitud y más concretamente con el Machine Learning, permite que el software que manejan las empresas puedan aprender, los patrones y comportamientos detectados en los clientes, tomando decisiones por sí mismos, pero para ello requiere de la orientación humana. En la actualidad, Deep Learning, una disciplina dentro del Machine Learning, está trabajando para conseguir que el aprendizaje del software sea totalmente autónomo, sin intervención humana y así conseguir el gran reto de simular cómo aprende el cerebro humano. La Analítica de datos y las técnicas de Inteligencia Artificial han existido desde hace muchos años, sin embargo, el auge actual se debe al Big Data, que está permitiendo gestionar volúmenes ingentes de información y su procesamiento de forma ágil; cuanto mayor sea el volumen de información, más acertados serán los patrones y comportamientos detectados, de ahí la importancia de disponer de cantidades ingentes de datos y capacidad para procesarla rápidamente e incluso en tiempo real.\\ La sinergia entre estas cuatro tecnologías, Analítica de datos, Machine Learning (o Deep Learning) y Big Data permiten a las empresas innovar en todas sus estructuras y ofrecer al cliente un servicio totalmente personalizado y a medida. Los datos son el petróleo del siglo XXI y estas tecnologías los explotan cuidadosamente para ofrecer servicios a medida y una nueva perspectiva, que ya demanda el cliente .

\section{Estado actual de la Inteligencia Artificial}

\subsection*{¿Qué puede hacer la IA hoy en día?}
\textbf{Jugar una partida decente de Tenis}
Los procesos de aprendizaje de robots son la base de proyectos como el robot Macgyver o de los brazos robóticos del MIT o un jugador de tenis de mesa.\\
Cuando el robot recibe la bola y debe responder, realiza una combinación de los 25 patrones mediante un sistema de ponderación, es decir, a cada uno de estos 25 movimientos base se les asigna un peso y el resultado final es el golpe que debe efectuar. ¿Y de dónde provienen los pesos? Aquí es donde entra en juego, de nuevo, el proceso de aprendizaje y entrenamiento, un proceso en el que se somete a prueba el sistema (se le hace jugar) y se le realimenta con los éxitos y fracasos en sus golpes, recalibrando los pesos y, mediante la práctica, obteniendo una mejor tasa de resultados.
\\\textbf{Conducir un coche}\\
El nuevo Mercedes-Benz Clase A 2018 tiene un asistente de voz que promete aprender del usuario gracias a la "inteligencia artificial", que tiene un panel de instrumentos cuyos gráficos ha diseñado NVIDIA y que se puede abrir con el smartphone. Es más, cualquiera con tu permiso puede abrir el coche con un smartphone.
\\\textbf{Comprar comestibles para una semana en la Web}
\\La inteligencia artificial ya es parte de los procesos de compras, entender tempranamente sus virtudes y limitantes, permitirá a las organizaciones obtener el mayor provecho de ella y adaptar las competencias de su personal a la nueva forma de trabajar.
 \\\textbf{Jugar una partida de damas chinas}
 \\Se dice que el juego de Damas chinas es un juego de mesa resuelto, es decir, que ya no hay forma de ganarle a una Inteligencia Arificial en este juego, lo mas a lo que podemos llegar es a un empate.
 \\\textbf{descubrir y demostrar nuevos teoremas matematicos}
\subsection*{¿Qué no puede hacer la IA hoy en día?}
\textbf{Comprar comestibles para una semana en el mercado}\\
no existen sensores para determinar la calidad y tipo de producto se estimula que en 25 años se pueda realizar.
\\\textbf{Escribir intencionalmente una historia divertida}\\Esta tarea es casi imposible pues una maquina no tiene la capacidad de crear nuevas ideas ni tiene una proyeccion del tiempo.
\\\textbf{Ofrecer asesoria legal competente en un area determinada}\\Un computador no tiene la capacidad de razonar
\\\textbf{Traducir ingles hablado al sueco hablado en tiempo real}\\El grado de dificultad es bastante alto pues previamente se deberia tener un conocimiento general del tema a tratar y ademas diferencie la pronunciacion y el significado de las palabras.
\\\textbf{Realizar una operacion de cirugia completa}\\Hoy en dia son solamente una ayuda pues se debe manejar una perfeccion en un area tan sensible como la vida humana se considera realizable en unos 40 años
\end{document}
%Á á, É é, Í í,Ó ó,Ú ú,Ü ü,Ñ ñ, ¿, ¡ ``